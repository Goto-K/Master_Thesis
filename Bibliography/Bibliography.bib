%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Saved with string encoding Unicode (UTF-8) 

@article{ILCTDRVES,
  title={{The International Linear Collider Technical Design Report - Volume 1: Executive Summary}},
  author={Behnke, Ties and Brau, James E and Foster, Brian and Fuster, Juan and Harrison, Mike and Paterson, James McEwan and Peskin, Michael and Stanitzki, Marcel and Walker, Nicholas and Yamamoto, Hitoshi},
  journal={arXiv preprint arXiv:1306.6327},
  year={2013}
}

@article{ILCTDRVP,
  title={{The International Linear Collider Technical Design Report - Volume 2: Physics}},
  author={Baer, Howard and Barklow, Tim and Fujii, Keisuke and Gao, Yuanning and Hoang, Andre and Kanemura, Shinya and List, Jenny and Logan, Heather E and Nomerotski, Andrei and Perelstein, Maxim and others},
  journal={arXiv preprint arXiv:1306.6352},
  year={2013}
}

@misc{ILCPHOTO,
author = {},
title = {ILC PHOTO GALLERY},
howpublished = {\url{http://ilcgallery.com/}},
month = {},
year = {},
}

@misc{iLCSoft,
    author = {},
    title = {iLCSoft},
    howpublished = {\url{https://github.com/iLCSoft}},
    month = {},
    year = {},
}	

@misc{LCFIPlus,
    author = {},
    title = {lcfiplus/LCFIPlus: Flavor tagging code for ILC detectors},
    howpublished = {\url{https://github.com/lcfiplus/LCFIPlus}},
    month = {},
    year = {},
}
	
@article{RecommendationsonILCProjectImplementation,
    author = "Desch, Klaus and others",
    collaboration = "{{KEK International Working Group}}",
    title = "{{Recommendations on ILC Project Implementation}}",
    month = "9",
    year = "2019"
}

@article{PatternRecognitionUsingGeneralizedPortraitMethod,
  title={{Pattern recognition using generalized portrait method}},
  author={Vapnik, Vladimir},
  journal={Automation and remote control},
  volume={24},
  pages={774--780},
  year={1963}
}

@inproceedings{TrainingAlgorithmforOptimalMarginClassifiers,
  title={{A training algorithm for optimal margin classifiers}},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}

@article{GlobalProject,
  title={{The International Linear Collider: A Global Project}},
  author={Bambade, Philip and Barklow, Tim and Behnke, Ties and Berggren, Mikael and Brau, James and Burrows, Philip and Denisov, Dmitri and Faus-Golfe, Angeles and Foster, Brian and Fujii, Keisuke and others},
  journal={arXiv preprint arXiv:1903.01629},
  year={2019}
}

@article{InterimDesignReport,
  title={{International Large Detector: Interim Design Report}},
  author={The ILD Collaboration},
  journal={arXiv preprint arXiv:2003.01116},
  year={2020}
}

@book{ZeroDeepLearning1,
    author = {斎藤 康毅},
    title={ゼロから作るDeep Learning: Pythonで学ぶディープラーニングの理論と実装},
    isbn={9784873117584},
    series={ゼロから作るDeep Learning},
    url={https://www.oreilly.co.jp/books/9784873117584/},
    month = "9",
    year={2016},
    publisher={オライリー・ジャパン}
}

@book{ZeroDeepLearning2,
    author = {斎藤 康毅},
    title={ゼロから作るDeep Learning 2: 自然言語処理編},
    isbn={9784873118369},
    series={ゼロから作るDeep Learning},
    url={https://www.oreilly.co.jp/books/9784873118369},
    month = "7",
    year={2018},
    publisher={オライリー・ジャパン}
}

@book{PythonMLPrograming,
    author    = "Raschka, Sebastian and クイープ and 福島, 真太朗",
    title     = "Python機械学習プログラミング : 達人データサイエンティストによる理論と実践",
    publisher = "インプレス",
    year      = "2016",
    series    = "Impress top gear",
    number    = "",
    URL       = "https://ci.nii.ac.jp/ncid/BB21521206"
}

@article{LCFIPlusPaper,
   title={{LCFIPlus: A framework for jet analysis in linear collider studies}},
   volume={808},
   ISSN={0168-9002},
   url={http://dx.doi.org/10.1016/j.nima.2015.11.054},
   DOI={10.1016/j.nima.2015.11.054},
   journal={Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
   publisher={Elsevier BV},
   author={Suehara, Taikan and Tanabe, Tomohiko},
   year={2016},
   month={Feb},
   pages={109\UTF{2013}116}
}

@article{TrackParametersLCIO,
    author = "Kramer, Thomas",
    title = "{{Track parameters in LCIO}}",
    reportNumber = "LC-DET-2006-004",
    month = "8",
    year = "2006"
}

@article{PerceptronPaper,
author="Rosenblatt, F.",
title="{{The perceptron: A probabilistic model for information storage and organization in the brain}}",
journal="Psyhological Review",
ISSN="",
publisher="",
year="1958",
month="",
volume="65",
number="6",
pages="386-408",
URL="https://ci.nii.ac.jp/naid/20001617891/",
DOI="",
}

@article{Dartmouth,
  added-at = {2020-07-20T00:00:00.000+0200},
  author = {McCarthy, John and Minsky, Marvin and Rochester, Nathaniel and Shannon, Claude E.},
  biburl = {https://www.bibsonomy.org/bibtex/2bd187621e003f7aed003c3ddd44acfbb/dblp},
  ee = {https://doi.org/10.1609/aimag.v27i4.1904},
  interhash = {f359fffd134596aa1f608cbd91f15c18},
  intrahash = {bd187621e003f7aed003c3ddd44acfbb},
  journal = {AI Magazine},
  keywords = {dblp},
  number = 4,
  pages = {12-14},
  timestamp = {2020-07-24T00:24:37.000+0200},
  title = {{A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.}},
  url = {http://dblp.uni-trier.de/db/journals/aim/aim27.html#McCarthyMRS06},
  volume = 27,
  year = 2006
}


@article{ApproximationSuperpositionsSigmoidalFunction,
author="CYBENKO, G.",
title="{{Approximation by superpositions of a sigmoidal function}}",
journal="Mathematics of Control, Signals and Systems",
ISSN="",
publisher="",
year="1989",
month="",
volume="2",
number="",
pages="303-314",
URL="https://ci.nii.ac.jp/naid/30024206829/",
DOI="10.1007/BF02551274",
}

@misc{TensorflowWeb,
author = {},
title = {TensorFlow},
howpublished = {\url{https://www.tensorflow.org/?hl=ja}},
month = {},
year = {},
}

@misc{KerasWeb,
author = {},
title = {Home - Keras Documentation},
howpublished = {\url{https://keras.io/ja/}},
month = {},
year = {},
}

@misc{PyTorchWeb,
author = {},
title = {PyTorch},
howpublished = {\url{https://pytorch.org/}},
month = {},
year = {},
}

@misc{ChainerWeb,
author = {},
title = {Chainer: A flexible framework for neural networks},
howpublished = {\url{https://chainer.org/}},
month = {},
year = {},
}

@misc{CaffeWeb,
author = {},
title = {Caffe | Deep Learning Framework},
howpublished = {\url{http://caffe.berkeleyvision.org/}},
month = {},
year = {},
}

@Article{SGD,
author = "B. Widrow and M. E. Hoff",
title = "{{Adaptive Switching Circuits}}",
journal = "1960 IRE WESCON Convention Record",
publisher = "IRE",
year = "1960",
pages = "96--104",
note = "Reprinted in {\sl Neurocomputing} MIT Press, 1988 .",
}

@misc{RMSProp,
  title={{Lecture 6.5---rmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}


@article{Adam,
  title={{Adam: A Method for Stochastic Optimization}},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={arXiv preprint arXiv:1412.6980},
  year={2017}
}


@article{Backpropagation,
	doi = {10.1038/323533a0},
	url = {https://doi.org/10.1038%2F323533a0},
	year = 1986,
	month = {oct},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {323},
	number = {6088},
	pages = {533--536},
	author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
	title = {{Learning representations by back-propagating errors}},
	journal = {Nature}
}

@article {Autoencoder,
	title = {{Reducing the Dimensionality of Data with Neural Networks}},
	volume = {313},
	number = {5786},
	pages = {504--507},
	author = {Geoffrey E. Hinton and Ruslan Russ Salakhutdinov},
	year = {2006},
	doi = {10.1126/science.1127647},
	publisher = {American Association for the Advancement of Science},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/313/5786/504},
	journal = {Science}
}


@article{GenerativeAdversarialNetworks,
  title={{Generative Adversarial Networks}},
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  journal={arXiv preprint arXiv:1406.2661},
  year={2014}
}


@article{LSTMpaper,
author = {Hochreiter, Sepp and Schmidhuber, J\UTF{00FC}rgen},
title = {{Long Short-Term Memory}},
journal = {Neural Computation},
volume = {9},
number = {8},
pages = {1735-1780},
year = {1997},
doi = {10.1162/neco.1997.9.8.1735},
URL = {https://doi.org/10.1162/neco.1997.9.8.1735},
eprint = {https://doi.org/10.1162/neco.1997.9.8.1735},
abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}


@article{GRU,
  title={{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
  author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{LuongAttention,
  title={{Effective Approaches to Attention-based Neural Machine Translation}},
  author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
  journal={arXiv preprint arXiv:1508.04025},
  year={2015}
}

@article{BahdanauAttention,
  title={{Neural Machine Translation by Jointly Learning to Align and Translate}},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={arXiv preprint arXiv:1409.0473},
  year={2016}
}


@article{AttentionIsAllYouNeed,
  title={{Attention Is All You Need}},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@misc{GitHubGotoK,
author = {},
title = {Goto-K/VertexFinderwithDL},
howpublished = {\url{https://github.com/Goto-K/VertexFinderwithDL}},
month = {},
year = {},
}

@misc{GitHubGotoKLCFIPlus,
author = {},
title = {Goto-K/LCFIPlus: Flavor tagging code for ILC detectors},
howpublished = {\url{https://github.com/Goto-K/LCFIPlus}},
month = {},
year = {},
note = {}
}

@article{WHIZARDpaper,
   title={{WHIZARD—simulating multi-particle processes at LHC and ILC}},
   volume={71},
   ISSN={1434-6052},
   url={http://dx.doi.org/10.1140/epjc/s10052-011-1742-y},
   DOI={10.1140/epjc/s10052-011-1742-y},
   number={9},
   journal={The European Physical Journal C},
   publisher={Springer Science and Business Media LLC},
   author={Kilian, Wolfgang and Ohl, Thorsten and Reuter, J\UTF{00FC}rgen},
   year={2011},
   month={Sep}
}

@article{BatchNormalizationpaper,
  title={{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
  author={Sergey Ioffe and Christian Szegedy},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@inproceedings{ReLUpaper,
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  biburl = {https://www.bibsonomy.org/bibtex/2059683ca9b2457d248942520babbe000/dblp},
  booktitle = {ICML},
  editor = {F\UTF{00FC}rnkranz, Johannes and Joachims, Thorsten},
  ee = {https://icml.cc/Conferences/2010/papers/432.pdf},
  interhash = {acefcb0a5d1a937232f02f3fe0d5ab86},
  intrahash = {059683ca9b2457d248942520babbe000},
  keywords = {dblp},
  pages = {807-814},
  publisher = {Omnipress},
  title = {{Rectified Linear Units Improve Restricted Boltzmann Machines.}},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2010.html#NairH10},
  year = 2010
}

@article{Marlinpaper,
title = "{{Marlin and LCCD—Software tools for the ILC}}",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "559",
number = "1",
pages = "177 - 180",
year = "2006",
note = "Proceedings of the X International Workshop on Advanced Computing and Analysis Techniques in Physics Research",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2005.11.138",
url = "http://www.sciencedirect.com/science/article/pii/S0168900205022643",
author = "F. Gaede",
keywords = "Simulation, Monte Carlo, Software tools, Linear collider",
abstract = "The next big project proposed in particle physics is the International Linear Collider (ILC), an electron positron collider with an energy reach of around 1TeV. The ongoing optimization and development of a detector for the ILC is only possible through the extensive use of sophisticated simulation software. In this paper we give a brief review of the software tools that are available in the currently ongoing three international detector concept studies and present two new software packages that have been developed in the context of the Large Detector Concept (LDC) study. The first is a C++ application framework that provides a platform for the distributed development of reconstruction and analysis software and the second is a conditions data toolkit. The interoperability with other software packages is discussed."
}

@article{DD4heppaper1,
	doi = {10.1088/1742-6596/513/2/022010},
	url = {https://doi.org/10.1088%2F1742-6596%2F513%2F2%2F022010},
	year = 2014,
	month = {jun},
	publisher = {{IOP} Publishing},
	volume = {513},
	number = {2},
	pages = {022010},
	author = {M Frank and F Gaede and C Grefe and P Mato},
	title = {{DD4hep: A Detector Description Toolkit for High Energy Physics Experiments}},
	journal = {Journal of Physics: Conference Series},
	abstract = {The detector description is an essential component that is used to analyze data resulting from particle collisions in high energy physics experiments. We will present a generic detector description toolkit and describe the guiding requirements and the architectural design for such a toolkit, as well as the main implementation choices. The design is strongly driven by easy of use; developers of detector descriptions and applications using them should provide minimal information and minimal specific code to achieve the desired result. The toolkit will be built reusing already existing components from the ROOT geometry package and provides missing functional elements and interfaces to offer a complete and coherent detector description solution. A natural integration to Geant4, the detector simulation program used in high energy physics, is provided.}
}

@article{DD4heppaper2,
	doi = {10.1088/1742-6596/664/7/072017},
	url = {https://doi.org/10.1088%2F1742-6596%2F664%2F7%2F072017},
	year = 2015,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {664},
	number = {7},
	pages = {072017},
	author = {M. Frank and F. Gaede and N. Nikiforou and M. Petric and A. Sailer},
	title = {{DDG4 A Simulation Framework based on the {DD}4hep Detector Description Toolkit}},
	journal = {Journal of Physics: Conference Series},
	abstract = {The detector description is an essential component that has to be used to analyse and simulate data resulting from particle collisions in high energy physics experiments. Based on the DD4hep detector description toolkit a flexible and data driven simulation framework was designed using the Geant4 tool-kit. We present this framework and describe the guiding requirements and the architectural design, which was strongly driven by ease of use. The goal was, given an existing detector description, to simulate the detector response to particle collisions in high energy physics experiments with minimal effort, but not impose restrictions to support enhanced or improved behaviour.

Starting from the ROOT based geometry implementation used by DD4hep an automatic conversion mechanism to Geant4 was developed. The physics response and the mechanism to input particle data from generators was highly formalized and can be instantiated on demand using known factory patterns. A palette of components to model the detector response is provided by default, but improved or more sophisticated components may easily be added using the factory pattern. Only the final configuration of the instantiated components has to be provided by end-users using either C++ or python scripting or an XML based description.}
}

@article{Durhampaper,
title = "{{New clustering algorithm for multijet cross sections in e+e− annihilation}}",
journal = "Physics Letters B",
volume = "269",
number = "3",
pages = "432 - 438",
year = "1991",
issn = "0370-2693",
doi = "https://doi.org/10.1016/0370-2693(91)90196-W",
url = "http://www.sciencedirect.com/science/article/pii/037026939190196W",
author = "S. Catani and Yu.L. Dokshitzer and M. Olsson and G. Turnock and B.R. Webber",
abstract = "Cross sections for e+e− → n-jets, as functions of the jet resolution parameter ycut, are computed according to a new clustering algorithm. The jet multiplicity n is defined in such a way that jets i and j with energies Ei and Ej at relative angle angle θij are resolved if yij = 2(1−cosθij) min (Ei2, Ej2)s>ycut, where s is the centre-of-mass energy squared. Using this algorithm, large higher-order corrections at small values of ycut can easily be evaluated. Our calculations include resummation of leading and next-to-leading logarithms of ycut to all orders in QCD perturbation theory. This enables us to predict the jet cross sections at small ycut for arbitrary n. Simple analytical results for n\UTF{2A7D}5 are presented."
}

@article{t-SNEpaper,
  added-at = {2015-06-19T12:07:15.000+0200},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/28b9aebb404ad4a4c6a436ea413550b30/lopusz_kdd},
  interhash = {370ba8b9e1909b61880a6f47c93bcd49},
  intrahash = {8b9aebb404ad4a4c6a436ea413550b30},
  journal = {Journal of Machine Learning Research},
  keywords = {dimensionality_reduction tSNE visualization},
  pages = {2579--2605},
  timestamp = {2015-08-19T15:19:11.000+0200},
  title = {{{Visualizing Data using t-SNE}}},
  url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
  volume = 9,
  year = 2008
}

