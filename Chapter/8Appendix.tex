% !TEX root = ../MasterThesis_goto_v1.tex

\appendix 

\chapter{ソースコード} \label{sec:Code}

ここでは本研究で使用したコードについて記述する。
ここで紹介する実装方法はTensorflow/Kerasを用いた一例である。
実装の解説については要点のみ述べる。
詳細なコードについては参考文献\cite{GitHubGotoK}を参照して頂きたい。

\section{任意の数の飛跡についてのネットワーク} \label{sec:CodeVLSTM}

まず, \ref{Net:VLSTM:PerformanceofVLSTM}で比較した3つのネットワークについて, 使用したコードを記述する。

\begin{lstlisting}[caption=標準的なLSTM, label=SimpleLSTM]
def LSTMModelSimple(pair, tracks, UNITS=256):

    MAX_TRACK_NUM = tracks.shape[1]
    INPUT_DIM = tracks.shape[2]
    PAIR_DIM = pair.shape[1]

    pair_input = Input(shape=(PAIR_DIM,), name='Pair_Input')

    track_input = Input(shape=(None, INPUT_DIM), name='Input')
    track_embedd = TimeDistributed(Dense(UNITS, name='Embedding_Dense', activation='relu'))(track_input)

    init_state = Dense(UNITS, name='Init_State_Dense_1')(pair_input)
    init_state = BatchNormalization(name='Init_State_BatchNorm_1')(init_state)
    init_state = Activation('relu', name='Init_State_Activation_1')(init_state)
    init_state = Dense(UNITS, name='Init_State_Dense_2')(init_state)
    init_state = BatchNormalization(name='Init_State_BatchNorm_2')(init_state)
    init_state = Activation('relu', name='Init_State_Activation_2')(init_state)

    rnn = LSTM(UNITS, return_sequences=True, name='LSTM_Simple')(track_embedd, initial_state=[init_state, init_state])
    rnn = TimeDistributed(Dense(1, name='Last_Dense', activation='relu'))(rnn)
    
    model = Model(inputs=[pair_input, track_input], outputs=rnn)

    model.summary()

    return model
\end{lstlisting}

\begin{lstlisting}[caption=独自のLSTM, label=DedicatedLSTM]
def VLSTMModelSimple(pair, tracks, UNITS=256):

    MAX_TRACK_NUM = tracks.shape[1]
    INPUT_DIM = tracks.shape[2]
    PAIR_DIM = pair.shape[1]

    pair_input = Input(shape=(PAIR_DIM,), name='Pair_Input')

    track_input = Input(shape=(None, INPUT_DIM), name='Input')
    track_embedd = TimeDistributed(Dense(UNITS, name='Embedding_Dense', activation='relu'))(track_input)

    init_state = Dense(UNITS, name='Init_State_Dense_1')(pair_input)
    init_state = BatchNormalization(name='Init_State_BatchNorm_1')(init_state)
    init_state = Activation('relu', name='Init_State_Activation_1')(init_state)
    init_state = Dense(UNITS, name='Init_State_Dense_2')(init_state)
    init_state = BatchNormalization(name='Init_State_BatchNorm_2')(init_state)
    init_state = Activation('relu', name='Init_State_Activation_2')(init_state)

    cell = layers.VLSTMCellSimple(UNITS, 1)

    rnn = RNN(cell, return_sequences=True, name='Vertex_LSTM_Simple')(track_embedd, initial_state=[init_state, init_state])
    
    model = Model(inputs=[pair_input, track_input], outputs=rnn)

    model.summary()

    return model
\end{lstlisting}

\begin{lstlisting}[caption=独自のAttention LSTM, label=DedicatedAttentionLSTM]
def AttentionVLSTMModel(pair, tracks, ENCODER_UNITS=256, DECODER_UNITS=256):

    MAX_TRACK_NUM = tracks.shape[1]
    INPUT_DIM = tracks.shape[2]
    PAIR_DIM = pair.shape[1]

    pair_input = Input(shape=(PAIR_DIM,), name='Pair_Input')

    encoder_input = Input(shape=(MAX_TRACK_NUM, INPUT_DIM), name='Encoder_Input')
    encoder_embedd = TimeDistributed(Dense(ENCODER_UNITS, name='Encoder_Embedding_Dense', activation='relu'))(encoder_input)

    decoder_input = Input(shape=(None, INPUT_DIM), name='Decoder_Input')
    decoder_embedd = TimeDistributed(Dense(DECODER_UNITS, name='Decoder_Embedding_Dense', activation='relu'))(decoder_input)

    encoder_init_state_f = Dense(ENCODER_UNITS, name='Encoder_Forward_Dense_1')(pair_input)
    ecnoder_init_state_f = BatchNormalization(name='Encoder_Forward_BatchNorm_1')(encoder_init_state_f)
    encoder_init_state_f = Activation('relu', name='Encoder_Forward_Activation_1')(encoder_init_state_f)
    encoder_init_state_f = Dense(ENCODER_UNITS, name='Encoder_Forward_Dense_2')(encoder_init_state_f)
    ecnoder_init_state_f = BatchNormalization(name='Encoder_Forward_BatchNorm_2')(encoder_init_state_f)
    encoder_init_state_f = Activation('relu', name='Encoder_Forward_Activation_2')(encoder_init_state_f)

    encoder_init_state_b = Dense(ENCODER_UNITS, name='Encoder_Backward_Dense_1')(pair_input)
    ecnoder_init_state_b = BatchNormalization(name='Encoder_Backward_BatchNorm_1')(encoder_init_state_b)
    encoder_init_state_b = Activation('relu', name='Encoder_Backward_Activation_1')(encoder_init_state_b)
    encoder_init_state_b = Dense(ENCODER_UNITS, name='Encoder_Backward_Dense_2')(encoder_init_state_b)
    ecnoder_init_state_b = BatchNormalization(name='Encoder_Backward_BatchNorm_2')(encoder_init_state_b)
    encoder_init_state_b = Activation('relu', name='Encoder_Backward_Activation_2')(encoder_init_state_b)

    decoder_init_state = Dense(DECODER_UNITS, name='Decoder_Dense_1')(pair_input)
    denoder_init_state = BatchNormalization(name='Decoder_BatchNorm_1')(decoder_init_state)
    decoder_init_state = Activation('relu', name='Decoder_Activation_1')(decoder_init_state)
    decoder_init_state = Dense(DECODER_UNITS, name='Decoder_Dense_2')(decoder_init_state)
    denoder_init_state = BatchNormalization(name='Decoder_BatchNorm_2')(decoder_init_state)
    decoder_init_state = Activation('relu', name='Decoder_Activation_2')(decoder_init_state)

    vlstm_cell_f = layers.VLSTMCellEncoder(ENCODER_UNITS)
    vlstm_cell_b = layers.VLSTMCellEncoder(ENCODER_UNITS)
    encoder_f = RNN(vlstm_cell_f, return_sequences=True, name="Encoder_Forward_VLSTM", go_backwards=False)
    encoder_b = RNN(vlstm_cell_b, return_sequences=True, name="Encoder_Backward_VLSTM", go_backwards=True)

    with CustomObjectScope({"VLSTMCellEncoder": layers.VLSTMCellEncoder}):
        biencoder = Bidirectional(encoder_f, backward_layer=encoder_b, name='Bidirectional_Encoder_VLSTM')(encoder_embedd, initial_state=[encoder_init_state_f, encoder_init_state_f, encoder_init_state_b, encoder_init_state_b])

    biencoder = Reshape(target_shape=(MAX_TRACK_NUM*ENCODER_UNITS*2,), name='Reshape_Bidirectional_Encoder')(biencoder)

    # DCODER_UNITS, ENCODER_UNITS, DECODER_OUTPUT, MAX_TRACK_NUM
    attentionvlstm_cell = layers.AttentionVLSTMCell(DECODER_UNITS, ENCODER_UNITS*2, 1, MAX_TRACK_NUM)
    
    decoder, attention = RNN(attentionvlstm_cell, return_sequences=True, name='Decoder_Attention_VLSTM')(decoder_embedd, initial_state=[biencoder, decoder_init_state])
    
    model = Model(inputs=[pair_input, encoder_input, decoder_input], outputs=decoder)

    model.summary()

    return model
\end{lstlisting}

コード\ref{SimpleLSTM}では特に独自機構は使用しておらず, 基本的にはTensorflow.Kerasの標準的なライブラリで動作している。
本研究ではRNNCellを用いて独自のリカレントニューラルネットワーク構造を定義している。
コード\ref{DedicatedLSTM}, \ref{DedicatedAttentionLSTM}では以下の様にCellを読み出し, 使用している。

\begin{lstlisting}[caption=独自のLSTM]
    cell = layers.VLSTMCellSimple(UNITS, 1)

    rnn = RNN(cell, return_sequences=True, name='Vertex_LSTM_Simple')(track_embedd, initial_state=[init_state, init_state])
\end{lstlisting}
\begin{lstlisting}[caption=独自のAttention LSTM]
    vlstm_cell_f = layers.VLSTMCellEncoder(ENCODER_UNITS)
    vlstm_cell_b = layers.VLSTMCellEncoder(ENCODER_UNITS)
    encoder_f = RNN(vlstm_cell_f, return_sequences=True, name="Encoder_Forward_VLSTM", go_backwards=False)
    encoder_b = RNN(vlstm_cell_b, return_sequences=True, name="Encoder_Backward_VLSTM", go_backwards=True)

    with CustomObjectScope({"VLSTMCellEncoder": layers.VLSTMCellEncoder}):
        biencoder = Bidirectional(encoder_f, backward_layer=encoder_b, name='Bidirectional_Encoder_VLSTM')(encoder_embedd, initial_state=[encoder_init_state_f, encoder_init_state_f, encoder_init_state_b, encoder_init_state_b])
\end{lstlisting}
\begin{lstlisting}
    attentionvlstm_cell = layers.AttentionVLSTMCell(DECODER_UNITS, ENCODER_UNITS*2, 1, MAX_TRACK_NUM)
    
    decoder, attention = RNN(attentionvlstm_cell, return_sequences=True, name='Decoder_Attention_VLSTM')(decoder_embedd, initial_state=[biencoder, decoder_init_state])
\end{lstlisting}

コード\ref{DedicatedLSTM}では"VLSTMCellSimple()"を, コード\ref{DedicatedAttentionLSTM}では"VLSTMCellEncoder()", "AttentionVLSTMCell()"をそれぞれ呼び出している。
"VLSTMCellSimple()"は図\ref{3-4-1-3Interpretation}を示しており, 以下の様に実装している。

\begin{lstlisting}[caption=独自のリカレントニューラルネットワーク構造の$1$ステップ, label=VLSTMCellSimple]
class VLSTMCellSimple(AbstractRNNCell):

  def __init__(self,
               units,
               units_out,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               dense_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):

    super(VLSTMCellSimple, self).__init__(**kwargs)
    self.units = units
    self.units_out = units_out
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.dense_activation = activations.get(dense_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    if implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation

  @property
  def state_size(self):
    return [self.units, self.units]

  def build(self, input_shape): # difinition of the weights
    input_dim = input_shape[-1]
    self.kernel = self.add_weight( # W
        shape=(input_dim, self.units * 4), # "* 4" means "o, f, i, z"
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)
    self.recurrent_kernel = self.add_weight( # R
        shape=(self.units, self.units * 4),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)
    self.dense_kernel = self.add_weight( # Last Dense kernel
        shape=(self.units * 1, self.units_out),
        name='dense_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    if self.use_bias:
      if self.unit_forget_bias:

        def bias_initializer(_, *args, **kwargs):
          return K.concatenate([
              self.bias_initializer((self.units,), *args, **kwargs),
              initializers.Ones()((self.units,), *args, **kwargs),
              self.bias_initializer((self.units * 2,), *args, **kwargs),
          ])
      else:
        bias_initializer = self.bias_initializer
      self.bias = self.add_weight( # b
          shape=(self.units * 4,),
          name='bias',
          initializer=bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    self.built = True

  def _compute_update_vertex(self, x, V_tm1):
    """Computes carry and output using split kernels."""
    # x = W * track
    x_i, x_f, x_c, x_o = x
    V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v = V_tm1
    # i = x_i + V_tm1_i * R_i
    #   = W_i * track + V_tm1_1 * R_i

    i = self.recurrent_activation(
        x_i + K.dot(V_tm1_i, self.recurrent_kernel[:, :self.units]))
    f = self.recurrent_activation(
        x_f + K.dot(V_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
    c = self.activation(
        x_c + K.dot(V_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))

    # U = update vertex
    U = f * V_tm1_u + i * c

    o = self.recurrent_activation(
        x_o + K.dot(V_tm1_o, self.recurrent_kernel[:, self.units * 3:]))

    h_temp = o * self.activation(V_tm1_o2)
    # h size [self.units]
    h = self.dense_activation(K.dot(h_temp, self.dense_kernel))
    # h size [1] activated sigmoid

    V = h * U + (1-h) * V_tm1_v
    return h, V

  def call(self, inputs, states, training=None):
    V_tm1 = states[0] # previous Vertex state

    if self.implementation == 1:
      # input = track
      inputs_i = inputs
      inputs_f = inputs
      inputs_c = inputs
      inputs_o = inputs
      # k = W
      k_i, k_f, k_c, k_o = array_ops.split(
              self.kernel, num_or_size_splits=4, axis=1)
      x_i = K.dot(inputs_i, k_i)
      x_f = K.dot(inputs_f, k_f)
      x_c = K.dot(inputs_c, k_c)
      x_o = K.dot(inputs_o, k_o)
      if self.use_bias:
        b_i, b_f, b_c, b_o = array_ops.split(
            self.bias, num_or_size_splits=4, axis=0)
        x_i = K.bias_add(x_i, b_i)
        x_f = K.bias_add(x_f, b_f)
        x_c = K.bias_add(x_c, b_c)
        x_o = K.bias_add(x_o, b_o)

      V_tm1_i = V_tm1
      V_tm1_f = V_tm1
      V_tm1_c = V_tm1
      V_tm1_o = V_tm1
      V_tm1_o2 = V_tm1
      V_tm1_u = V_tm1
      V_tm1_v = V_tm1
      x = (x_i, x_f, x_c, x_o)

      V_tm1 = (V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v)
      h, V = self._compute_update_vertex(x, V_tm1)

    return h, [V, V]

  def get_config(self):
    config = {
        'units':
            self.units,
        'units_out':
            self.units_out,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(VLSTMCellSimple, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
\end{lstlisting}

ここでは, 学習可能な重み行列$W_f, W_i, W_c, W_o, R_f, R_i, R_c, R_o, {\mbox{\boldmath{$d$}}}_h$は以下で定義されている。

\begin{lstlisting}[caption=独自のリカレントニューラルネットワーク構造の$1$ステップ, label=VLSTMCellSimple]
  def build(self, input_shape): # difinition of the weights
    input_dim = input_shape[-1]
    self.kernel = self.add_weight( # W
        shape=(input_dim, self.units * 4), # "* 4" means "o, f, i, z"
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)
    self.recurrent_kernel = self.add_weight( # R
        shape=(self.units, self.units * 4),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)
    self.dense_kernel = self.add_weight( # Last Dense kernel
        shape=(self.units * 1, self.units_out),
        name='dense_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    if self.use_bias:
      if self.unit_forget_bias:

        def bias_initializer(_, *args, **kwargs):
          return K.concatenate([
              self.bias_initializer((self.units,), *args, **kwargs),
              initializers.Ones()((self.units,), *args, **kwargs),
              self.bias_initializer((self.units * 2,), *args, **kwargs),
          ])
      else:
        bias_initializer = self.bias_initializer
      self.bias = self.add_weight( # b
          shape=(self.units * 4,),
          name='bias',
          initializer=bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    self.built = True
\end{lstlisting}

また, $1$ステップの演算は以下の様に行われる。

\begin{lstlisting}[caption=独自のリカレントニューラルネットワーク構造の$1$ステップ, label=VLSTMCellSimple]
  def _compute_update_vertex(self, x, V_tm1):
    """Computes carry and output using split kernels."""
    # x = W * track
    x_i, x_f, x_c, x_o = x
    V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v = V_tm1
    # i = x_i + V_tm1_i * R_i
    #   = W_i * track + V_tm1_1 * R_i

    i = self.recurrent_activation(
        x_i + K.dot(V_tm1_i, self.recurrent_kernel[:, :self.units]))
    f = self.recurrent_activation(
        x_f + K.dot(V_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
    c = self.activation(
        x_c + K.dot(V_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))

    # U = update vertex
    U = f * V_tm1_u + i * c

    o = self.recurrent_activation(
        x_o + K.dot(V_tm1_o, self.recurrent_kernel[:, self.units * 3:]))

    h_temp = o * self.activation(V_tm1_o2)
    # h size [self.units]
    h = self.dense_activation(K.dot(h_temp, self.dense_kernel))
    # h size [1] activated sigmoid

    V = h * U + (1-h) * V_tm1_v
    return h, V

  def call(self, inputs, states, training=None):
    V_tm1 = states[0] # previous Vertex state

    if self.implementation == 1:
      # input = track
      inputs_i = inputs
      inputs_f = inputs
      inputs_c = inputs
      inputs_o = inputs
      # k = W
      k_i, k_f, k_c, k_o = array_ops.split(
              self.kernel, num_or_size_splits=4, axis=1)
      x_i = K.dot(inputs_i, k_i)
      x_f = K.dot(inputs_f, k_f)
      x_c = K.dot(inputs_c, k_c)
      x_o = K.dot(inputs_o, k_o)
      if self.use_bias:
        b_i, b_f, b_c, b_o = array_ops.split(
            self.bias, num_or_size_splits=4, axis=0)
        x_i = K.bias_add(x_i, b_i)
        x_f = K.bias_add(x_f, b_f)
        x_c = K.bias_add(x_c, b_c)
        x_o = K.bias_add(x_o, b_o)

      V_tm1_i = V_tm1
      V_tm1_f = V_tm1
      V_tm1_c = V_tm1
      V_tm1_o = V_tm1
      V_tm1_o2 = V_tm1
      V_tm1_u = V_tm1
      V_tm1_v = V_tm1
      x = (x_i, x_f, x_c, x_o)

      V_tm1 = (V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v)
      h, V = self._compute_update_vertex(x, V_tm1)

    return h, [V, V]
\end{lstlisting}

以上が図\ref{3-4-1-3Interpretation}に示したネットワークである。
次に, これらをエンコーダー・デコーダーモデルへと拡張した際の変更点について実装をまとめる。
まずエンコーダー部については, デコーダー部へより多くの情報を受け渡す為, 重み${\mbox{\boldmath{$d$}}}_h$を取り除いている。

\begin{lstlisting}[caption=エンコーダー部の変更点, label=Encoder]
class VLSTMCellEncoder(AbstractRNNCell):
  def build(self, input_shape): # difinition of the weights
    input_dim = input_shape[-1]
    self.batch_size = input_shape[0]
    self.kernel = self.add_weight( # W
        shape=(input_dim, self.units * 4), # "* 4" means "o, f, i, z"
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)
    self.recurrent_kernel = self.add_weight( # R
        shape=(self.units, self.units * 4),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    if self.use_bias:
      if self.unit_forget_bias:

        def bias_initializer(_, *args, **kwargs):
          return K.concatenate([
              self.bias_initializer((self.units,), *args, **kwargs),
              initializers.Ones()((self.units,), *args, **kwargs),
              self.bias_initializer((self.units * 2,), *args, **kwargs),
          ])
      else:
        bias_initializer = self.bias_initializer
      self.bias = self.add_weight( # b
          shape=(self.units * 4,),
          name='bias',
          initializer=bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    self.built = True

  def _compute_update_vertex(self, x, V_tm1):
    """Computes carry and output using split kernels."""
    x_i, x_f, x_c, x_o = x
    V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v = V_tm1

    i = self.recurrent_activation(
        x_i + K.dot(V_tm1_i, self.recurrent_kernel[:, :self.units]))
    f = self.recurrent_activation(
        x_f + K.dot(V_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
    c = self.activation(
        x_c + K.dot(V_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))

    U = f * V_tm1_u + i * c

    o = self.recurrent_activation(
        x_o + K.dot(V_tm1_o, self.recurrent_kernel[:, self.units * 3:]))

    h = o * self.activation(V_tm1_o2)

    V = h * U + (1-h) * V_tm1_v
    #the size of h is [self.units]
    return h, V

  def call(self, inputs, states, training=None):
    V_tm1 = states[0] # previous Vertex state


    if self.implementation == 1:
      inputs_i = inputs
      inputs_f = inputs
      inputs_c = inputs
      inputs_o = inputs
      k_i, k_f, k_c, k_o = array_ops.split(
              self.kernel, num_or_size_splits=4, axis=1)
      x_i = K.dot(inputs_i, k_i)
      x_f = K.dot(inputs_f, k_f)
      x_c = K.dot(inputs_c, k_c)
      x_o = K.dot(inputs_o, k_o)
      if self.use_bias:
        b_i, b_f, b_c, b_o = array_ops.split(
            self.bias, num_or_size_splits=4, axis=0)
        x_i = K.bias_add(x_i, b_i)
        x_f = K.bias_add(x_f, b_f)
        x_c = K.bias_add(x_c, b_c)
        x_o = K.bias_add(x_o, b_o)

      V_tm1_i = V_tm1
      V_tm1_f = V_tm1
      V_tm1_c = V_tm1
      V_tm1_o = V_tm1
      V_tm1_o2 = V_tm1
      V_tm1_u = V_tm1
      V_tm1_v = V_tm1
      x = (x_i, x_f, x_c, x_o)

      V_tm1 = (V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v)
      h, V = self._compute_update_vertex(x, V_tm1)

    return h, [V, V]
\end{lstlisting}

デコーダー部ではAttentionへ拡張する為, 重み行列${\mbox{\boldmath{$u$}}}_{\rm energy},\  U_{\rm key},\ U_{\rm query}$, $C_f,\ C_c,\ C_i,\ C_o$を追加している。

\begin{lstlisting}[caption=デコーダー部の変更点, label=Decoder]
  def build(self, input_shape): # difinition of the weights
    self.feature_dim = input_shape[-1]
    self.batch_size = input_shape[0]

    self.attention_kernel_V = self.add_weight(　# u_energy
        shape=(self.units,),
        name='attention_kernel_V',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)

    self.attention_kernel_W = self.add_weight( # U_Query
        shape=(self.feature_dim, self.units),
        name='attention_kernel_W',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)

    self.attention_kernel_U = self.add_weight( # U_Key
        shape=(self.att_input_dim, self.units),
        name='attention_kernel_U',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)

    self.attention_kernel_b = self.add_weight( # U_Key bias
        shape=(self.units,),
        name='attention_kernel_b',
        initializer=self.bias_initializer,
        regularizer=self.bias_regularizer,
        constraint=self.bias_constraint)

    self.kernel = self.add_weight( # W
        shape=(self.feature_dim, self.units * 4), # "* 4" means "o, f, i, z"
        name='att_kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)

    self.recurrent_kernel = self.add_weight( # R
        shape=(self.units, self.units * 4),
        name='att_recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    self.dense_kernel = self.add_weight( # Last Dense kernel
        shape=(self.units * 1, self.output_dim),
        name='att_dense_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    self.context_kernel = self.add_weight( # C
        shape=(self.att_input_dim, self.units * 4),
        name='att_recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    if self.use_bias:
      if self.unit_forget_bias:

        def bias_initializer(_, *args, **kwargs):
          return K.concatenate([
              self.bias_initializer((self.units,), *args, **kwargs),
              initializers.Ones()((self.units,), *args, **kwargs),
              self.bias_initializer((self.units * 2,), *args, **kwargs),
          ])
      else:
        bias_initializer = self.bias_initializer
      self.bias = self.add_weight( # b
          shape=(self.units * 4,),
          name='att_bias',
          initializer=bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    self.built = True
\end{lstlisting}

演算についても以下の様に変更している。

\begin{lstlisting}
  @tf.function
  def _time_distributed_dense(self, x, w, b=None, dropout=None,
                              input_dim=None, output_dim=None,
                              timesteps=None, training=None):
    if input_dim is None:
        input_dim = K.shape(x)[2]
    if timesteps is None:
        timesteps = K.shape(x)[1]
    if output_dim is None:
        output_dim = K.shape(w)[1]

    if dropout is not None and 0. < dropout < 1.:
        # apply the same dropout pattern at every timestep
        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))
        dropout_matrix = K.dropout(ones, dropout)
        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)
        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)

    # collapse time dimension and batch dimension together
    x = K.reshape(x, (-1, input_dim))
    x = K.dot(x, w)
    if b is not None:
        x = K.bias_add(x, b)
    # reshape to 3D tensor
    if K.backend() == 'tensorflow':
        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))
        x.set_shape([None, None, output_dim])
    else:
        x = K.reshape(x, (-1, timesteps, output_dim))
    return x

  def _compute_update_vertex(self, x, V_tm1, c):
    """Computes carry and output using split kernels."""
    x_i, x_f, x_c, x_o = x
    V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v = V_tm1
    c_i, c_f, c_c, c_o = c

    i = self.recurrent_activation(
        x_i
        + K.dot(V_tm1_i, self.recurrent_kernel[:, :self.units])
        + K.dot(c_i, self.context_kernel[:, :self.units])) # Attention information

    f = self.recurrent_activation(
        x_f
        + K.dot(V_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2])
        + K.dot(c_f, self.context_kernel[:, self.units:self.units * 2]))

    c = self.activation(
        x_c
        + K.dot(V_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3])
        + K.dot(c_c, self.context_kernel[:, self.units * 2:self.units * 3]))

    U = f * V_tm1_u + i * c

    o = self.recurrent_activation(
        x_o
        + K.dot(V_tm1_o, self.recurrent_kernel[:, self.units * 3:])
        + K.dot(c_o, self.context_kernel[:, self.units * 3:]))

    h_temp = o * self.activation(V_tm1_o2)
    # h size [self.units]
    h = self.dense_activation(K.dot(h_temp, self.dense_kernel))
    # h size [1] activated sigmoid

    V = h * U + (1-h) * V_tm1_v
    return h, V

  def call(self, inputs, states, training=None):
    # store the whole sequence so we can "attend" to it at each timestep

    att = states[0] # Attention input (track num, input dim) / key
    self.x_seq = K.reshape(att, (-1, self.timestep, self.att_input_dim)) # Attention input (track num, input dim)
    V_tm1 = states[1] # previous Vertex state (units)

    # Additive Attention Bahdanau et al., 2015
    self._uxpb = self._time_distributed_dense(self.x_seq,
                                              self.attention_kernel_U,
                                              b=self.attention_kernel_b,
                                              input_dim=self.att_input_dim,
                                              timesteps=self.timestep,
                                              output_dim=self.units)

    # repeat the input track to the length of the sequence (track num, feature dim))
    _tt = K.repeat(inputs, self.timestep) # inputs / query
    _Wxtt = K.dot(_tt, self.attention_kernel_W)
    et = K.dot(activations.tanh(_Wxtt + self._uxpb), K.expand_dims(self.attention_kernel_V)) # Energy

    """
    #Dot-Product Attention Luong et al., 2015 / Scaled Dot-Product Attention Vaswani 2017
    self.x_seq /= np.sqrt(self.att_input_dim)
    et = K.batch_dot(K.expand_dims(inputs), self.x_seq, axes=[1, 2])
    et = K.reshape(et, (-1, self.timestep, 1))
    """

    at = K.exp(et)
    at_sum = K.sum(at, axis=1)
    at_sum_repeated = K.repeat(at_sum, self.timestep)
    at /= at_sum_repeated  # attention weights ({batchsize}, track num, 1)
    context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1) # context


    if self.implementation == 1:
      inputs_i = inputs
      inputs_f = inputs
      inputs_c = inputs
      inputs_o = inputs
      k_i, k_f, k_c, k_o = array_ops.split(
              self.kernel, num_or_size_splits=4, axis=1)
      x_i = K.dot(inputs_i, k_i)
      x_f = K.dot(inputs_f, k_f)
      x_c = K.dot(inputs_c, k_c)
      x_o = K.dot(inputs_o, k_o)
      if self.use_bias:
        b_i, b_f, b_c, b_o = array_ops.split(
            self.bias, num_or_size_splits=4, axis=0)
        x_i = K.bias_add(x_i, b_i)
        x_f = K.bias_add(x_f, b_f)
        x_c = K.bias_add(x_c, b_c)
        x_o = K.bias_add(x_o, b_o)

      V_tm1_i = V_tm1
      V_tm1_f = V_tm1
      V_tm1_c = V_tm1
      V_tm1_o = V_tm1
      V_tm1_o2 = V_tm1
      V_tm1_u = V_tm1
      V_tm1_v = V_tm1

      c_i = context
      c_f = context
      c_c = context
      c_o = context

      x = (x_i, x_f, x_c, x_o)
      V_tm1 = (V_tm1_i, V_tm1_f, V_tm1_c, V_tm1_o, V_tm1_o2, V_tm1_u, V_tm1_v)
      c = (c_i, c_f, c_c, c_o)

      h, V = self._compute_update_vertex(x, V_tm1, c)

    return [h, at], [att, V]
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{崩壊点検出アルゴリズム} \label{sec:CodePairProduction}

ここでは崩壊点検出アルゴリズムの内, Pythonを用いた実装についてまとめる。
崩壊点検出アルゴリズムは図\ref{4-1-0-1VertexFinderAlgorithm}に示したとおりである。

\begin{lstlisting}[caption=崩壊点検出アルゴリズム用関数,label=VertexFinder1]
def ModelsLoad(pair_model_name, lstm_model_name, slstm_model_name):

    pair_model = modeltools.LoadPairModel(pair_model_name)
    lstm_model = modeltools.LoadAttentionVLSTMModel(lstm_model_name)
    slstm_model = modeltools.LoadAttentionVLSTMModel(slstm_model_name)
    pair_model.compile(loss={'Vertex_Output': 'categorical_crossentropy', 'Position_Output': 'mean_squared_logarithmic_error'},
                       optimizer='SGD',
                       metrics=['accuracy', 'mae'])
    lstm_model.compile(loss='binary_crossentropy',
                       optimizer="Adam",
                       metrics=['accuracy'])
    slstm_model.compile(loss='binary_crossentropy',
                        optimizer="Adam",
                        metrics=['accuracy'])

    return pair_model, lstm_model, slstm_model


def PairInference(pair_model, variables):

    predict_vertex, predict_position = pair_model.predict([variables], verbose=1)

    return predict_vertex, predict_position


def GetEncoderDecoderTracksandTrue(debug, event_data, NTrack, MaxTrack):

    tracks = []
    true_label = []
    chain_lists = []
    particle_lists = []
    chain_label = [0 for i in range(int(NTrack))]
    vertex_mat_tbbcc = [[0 for j in range(int(NTrack))] for i in range(int(NTrack))]
    vertex_mat_tbc = [[0 for j in range(int(NTrack))] for i in range(int(NTrack))]
    
    for event_datum in event_data:
        if event_datum[57] == 2 or event_datum[57] == 3 or event_datum[57] == 4:
            vertex_mat_tbbcc[int(event_datum[1])][int(event_datum[2])] = 1
            vertex_mat_tbbcc[int(event_datum[2])][int(event_datum[1])] = 1
        if event_datum[57] == 3 or event_datum[57] == 4 or event_datum[57] == 5:
            vertex_mat_tbc[int(event_datum[1])][int(event_datum[2])] = 1
            vertex_mat_tbc[int(event_datum[2])][int(event_datum[1])] = 1
        
        if int(event_datum[1]) == int(NTrack)-1:
            
            if(event_datum[71]==1): true_label.append("c")
            elif(event_datum[72]==1): true_label.append("b")
            elif(event_datum[73]==1): true_label.append("o")
            elif(event_datum[74]==1): true_label.append("p")
            
            tracks.append(np.concatenate([[1], event_datum[25:47]]))
            if int(event_datum[2]) == int(NTrack)-2:
             
                if(event_datum[63]==1): true_label.append("c")
                elif(event_datum[64]==1): true_label.append("b")
                elif(event_datum[65]==1): true_label.append("o")
                elif(event_datum[66]==1): true_label.append("p")
            
                tracks.append(np.concatenate([[1], event_datum[3:25]]))

    decoder_tracks = np.array(deepcopy(tracks))
    encoder_tracks = np.pad(np.array(deepcopy(tracks)), [(0, int(MaxTrack-NTrack)), (0, 0)])

    vertex_mat_tbbcc = np.array(vertex_mat_tbbcc)
    vertex_mat_tbc = np.array(vertex_mat_tbc)

    for t in range(int(NTrack)):
        vertex_mat_tbbcc[t][t] = 1
        vertex_mat_tbc[t][t] = 1
        tmp_particle_lists= [part for particle in particle_lists for part in particle]
        tmp_chain_lists= [ch for chain in chain_lists for ch in chain]
        particle_list = [i for i, x in enumerate(vertex_mat_tbbcc[t, :]) if x == 1]
        chain_list = [i for i, x in enumerate(vertex_mat_tbc[t, :]) if x == 1]
        if len(particle_list) > 1 and (t not in tmp_particle_lists): particle_lists.append(particle_list)
        if len(chain_list) > 1 and (t not in tmp_chain_lists): chain_lists.append(chain_list)

    for i, particle_list in enumerate(particle_lists):
        for particle in particle_list:
            chain_label[int(particle)] = -(i+1)
    
    for i, chain_list in enumerate(chain_lists):
        c = i + 1
        for chain in chain_list:
            if chain_label[int(chain)] > 0: c = chain_label[int(chain)]
        for chain in chain_list:
            chain_label[int(chain)] = c

    if debug==True: 
        print("Encoder Trach Shape " + str(encoder_tracks.shape))
        print("Decoder Trach Shape " + str(decoder_tracks.shape))
        print("True Label" + str(true_label))
        print("Chain Label" + str(chain_label))
        print(list(particle_lists))
        print(list(chain_lists))

    return encoder_tracks, decoder_tracks, true_label, chain_label


def SecondarySeedSelectionOne(debug, event_data, ThresholdPairSecondaryScore, ThresholdPairPosScore):

    predict_vertex_labels = np.argmax(event_data[:, -8:-1], axis=1)
    secondary_event_data = []
    tmp_secondary_scores = []
    for event_datum, predict_vertex_label in zip(event_data, predict_vertex_labels):
        tmp_secondary_score = event_datum[-6] + event_datum[-5] + event_datum[-4] + event_datum[-3]
        if predict_vertex_label==0 or predict_vertex_label==1 or  predict_vertex_label==6: continue
        if tmp_secondary_score < ThresholdPairSecondaryScore: continue
        if event_datum[-1] > ThresholdPairPosScore: continue
	
        secondary_event_data.append(event_datum)
        tmp_secondary_scores.append(tmp_secondary_score)
        
    tmp_secondary_scores = np.array(tmp_secondary_scores)
    secondary_event_data = np.array(secondary_event_data)
    index = np.argsort(-tmp_secondary_scores)

    if debug==True: 
        for i, secondary_event_datum in enumerate(secondary_event_data[index]):
            print("Secondary Seeds " + str(i) + " Track 1: " + str(secondary_event_datum[1]) + " Track 2: " + str(secondary_event_datum[2]) + " SV Score: " + str(secondary_event_datum[-6] + secondary_event_datum[-5] + secondary_event_datum[-4] + secondary_event_datum[-3]))
    
    return secondary_event_data[index]


def PrimaryVertexFinder(debug, MaxPrimaryVertexLoop, ThresholdPrimaryScore, event_data, 
                        encoder_tracks, decoder_tracks, lstm_model):

    primary_track_list = []
    bigger_primary_scores = []
    primary_pairs = []
    for event_datum in event_data[np.argsort(-event_data[:, -7])][:MaxPrimaryVertexLoop]:
        primary_pairs.append(event_datum[3:47])
    primary_encoder_tracks = np.tile(encoder_tracks, (MaxPrimaryVertexLoop, 1, 1)) # tracks.shape = (MaxPrimaryVertexLoop, MaxTrack, 23)
    primary_decoder_tracks = np.tile(decoder_tracks, (MaxPrimaryVertexLoop, 1, 1)) # tracks.shape = (MaxPrimaryVertexLoop, NTrack, 23)
    if debug==True: print("Primary Encoder Trach Shape " + str(primary_encoder_tracks.shape))
    if debug==True: print("Primary Decoder Trach Shape " + str(primary_decoder_tracks.shape))
    primary_scores = lstm_model.predict([primary_pairs, primary_encoder_tracks, primary_decoder_tracks])

    for i in range(len(primary_scores[0])): 
        tmpbigger_primary_scores = 0
        for j in range(len(primary_scores)):
            score = primary_scores[j][i]
            if tmpbigger_primary_scores < score: tmpbigger_primary_scores = score
        
        bigger_primary_scores.append(tmpbigger_primary_scores)
        if tmpbigger_primary_scores < ThresholdPrimaryScore: continue
        if debug==True: print("Track " + str(i) + " Primary Score: " + str(tmpbigger_primary_scores))
        primary_track_list.append(i)

    return primary_track_list, bigger_primary_scores


def SecondaryVertexFinder(debug, ThresholdSecondaryScore, bigger_primary_scores, primary_track_list, secondary_event_data, 
                          encoder_tracks, decoder_tracks, slstm_model):

    track_list = np.arange(decoder_tracks.shape[0])
    secondary_track_lists = []
    for secondary_event_datum in secondary_event_data:
        track1, track2 = secondary_event_datum[1], secondary_event_datum[2]
        if (track1 in primary_track_list) or (track2 in primary_track_list): continue
        if (track1 not in list(track_list)) or (track2 not in list(track_list)): continue

        #if debug==True: print("Track List " + str(track_list))
        
        remain_decoder_tracks = decoder_tracks[track_list]
        secondary_pair = np.tile(secondary_event_datum[3:47], (1, 1)) # pair.shape = (1, 44)
        secondary_encoder_tracks = np.tile(encoder_tracks, (1, 1, 1)) # tracks.shape = (1, MaxTrack, 23)
        secondary_decoder_tracks = np.tile(remain_decoder_tracks, (1, 1, 1)) # tracks.shape = (1, RemainNTrack, 23)
        secondary_scores = slstm_model.predict([secondary_pair, secondary_encoder_tracks, secondary_decoder_tracks])
        secondary_scores = np.array(secondary_scores).reshape((-1, 1))

        tmptrack_list = np.copy(track_list)
        primary_track_list = np.array(primary_track_list, dtype=int)
        tmpsecondary_track_list = []
        for i, t in enumerate(track_list):
            if secondary_scores[i] > ThresholdSecondaryScore:
                if t not in primary_track_list:
                    tmpsecondary_track_list.append(t)
                    tmptrack_list = tmptrack_list[~(tmptrack_list == t)]
                elif (t in primary_track_list) and (secondary_scores[i] > bigger_primary_scores[t]):
                    tmpsecondary_track_list.append(t)
                    if debug==True:
                        print("Scramble Track Number " + str(t) + " SV Score " + str(secondary_score[i]) 
                              + " PV Score " + str(bigger_primary_scores[t]))
                    primary_track_list = primary_track_list[~(primary_track_list == t)]
                    tmptrack_list = tmptrack_list[~(tmptrack_list == t)]
        if len(tmpsecondary_track_list)!=0: secondary_track_lists.append(tmpsecondary_track_list)
        track_list = np.copy(tmptrack_list)

    return primary_track_list, secondary_track_lists


def CountPrintTrueTrackLists(debug, true_label, chain_label):

    ccbbvtx = 0
    bcvtx = 0
    true_secondary_bb_track_lists = []
    true_secondary_cc_track_lists = []
    true_secondary_same_chain_track_lists = []

    print(pycolor.YELLOW + "True Primary Vertex" + pycolor.END)
    true_primary_track_list = [i for i, x in enumerate(true_label) if x == "p"]
    print(pycolor.YELLOW + str(true_primary_track_list) + pycolor.END)

    for vtx in list(set(chain_label)):
        tcc = [i for i, (x, c) in enumerate(zip(true_label, chain_label)) if x == "c" and c == vtx]
        tbb = [i for i, (x, c) in enumerate(zip(true_label, chain_label)) if x == "b" and c == vtx]
        if vtx < 0:
            ccbbvtx = ccbbvtx + 1
            if len(tcc) != 0:
                print(pycolor.CYAN + "True Secondary Vertex Alone " + str(ccbbvtx) + pycolor.END)
                print(pycolor.CYAN + "cc : " + str(tcc) + pycolor.END)
                true_secondary_cc_track_lists.append(tcc)
                
            if len(tbb) != 0:
                print(pycolor.CYAN + "True Secondary Vertex Alone " + str(ccbbvtx) + pycolor.END)
                print(pycolor.CYAN + "bb : " + str(tbb) + pycolor.END)
                true_secondary_bb_track_lists.append(tbb)

        elif vtx > 0:
            bcvtx = ccbbvtx + 1
            print(pycolor.CYAN + "True Secondary Vertex Chain " + str(bcvtx) + pycolor.END)
            print(pycolor.CYAN + "cc : " + str(tcc) + pycolor.END)
            print(pycolor.CYAN + "bb : " + str(tbb) + pycolor.END)
            true_secondary_cc_track_lists.append(tcc)
            true_secondary_bb_track_lists.append(tbb)
            true_secondary_same_chain_track_lists.append([i for i, c in enumerate(chain_label) if c == vtx])

    print(pycolor.YELLOW + "True Other Tracks" + pycolor.END)
    true_other_track_list = [i for i, x in enumerate(true_label) if x == "o"]
    print(pycolor.YELLOW + str(true_other_track_list) + pycolor.END)

    return true_primary_track_list, true_secondary_bb_track_lists, true_secondary_cc_track_lists, true_secondary_same_chain_track_lists, true_other_track_list

        
def PrintPredTrackLists(primary_track_list, secondary_track_lists):

    print("Predict Primary Vertex")
    print(list(primary_track_list))

    for i, secondary_track_list in enumerate(secondary_track_lists):
        print("Predict Secondary Vertex " + str(i))
        print(list(secondary_track_list))


def yinx(y, x):
    for _y in y:
        if _y not in x: return False
    return True


def listremove(y, x):
    for _y in y:
        x.remove(_y)
    return x


def EvalPrintResults(debug, secondary_track_lists, true_primary_tracks, true_secondary_bb_track_lists, true_secondary_cc_track_lists, true_secondary_same_chain_track_lists, true_other_tracks,
		     MCPrimaryRecoSV, MCOthersRecoSV, MCBottomRecoSV, MCBottomRecoSVSameChain, MCBottomRecoSVSameParticle, MCCharmRecoSV, MCCharmRecoSVSameChain, MCCharmRecoSVSameParticle,
	             NumPVEvent, NumOthersEvent, NumBBEvent, NumCCEvent,
                     MCPrimaryRecoSVTrack, MCOthersRecoSVTrack, MCBottomRecoSVTrack, MCBottomRecoSVSameChainTrack, MCBottomRecoSVSameParticleTrack,
                     MCCharmRecoSVTrack, MCCharmRecoSVSameChainTrack, MCCharmRecoSVSameParticleTrack,
                     NumPVTrack, NumOthersTrack, NumBBTrack, NumCCTrack):

    true_secondary_bb_tracks = [track for tracks in true_secondary_bb_track_lists for track in tracks]
    true_secondary_cc_tracks = [track for tracks in true_secondary_cc_track_lists for track in tracks]
    true_secondary_same_particle_track_lists = true_secondary_bb_track_lists + true_secondary_cc_track_lists
    secondary_tracks = [track for tracks in secondary_track_lists for track in tracks]


    tmpMCPrimaryRecoSV = 0
    tmpMCOthersRecoSV = 0
    tmpMCBottomRecoSV = 0
    tmpMCBottomRecoSVSameChain = 0
    tmpMCBottomRecoSVSameParticle = 0
    tmpMCCharmRecoSV = 0
    tmpMCCharmRecoSVSameChain = 0
    tmpMCCharmRecoSVSameParticle = 0

    chains = deepcopy(secondary_tracks)
    particles = deepcopy(secondary_tracks)
    for secondary_track_list in secondary_track_lists:
        if len(secondary_track_list) == 0: continue
        chain_TrueorFalse = []
        particle_TrueorFalse = []
        for true_secondary_same_chain_track_list in true_secondary_same_chain_track_lists:
            chain_TrueorFalse.append(yinx(secondary_track_list, true_secondary_same_chain_track_list))
        if not any(chain_TrueorFalse): chains = listremove(secondary_track_list, chains)
        for true_secondary_same_particle_track_list in true_secondary_same_particle_track_lists:
            particle_TrueorFalse.append(yinx(secondary_track_list, true_secondary_same_particle_track_list))
        if not any(particle_TrueorFalse): particles = listremove(secondary_track_list, particles)

    if len(true_primary_tracks) != 0:
        for true_primary_track in true_primary_tracks:
            if true_primary_track in secondary_tracks: tmpMCPrimaryRecoSV = tmpMCPrimaryRecoSV + 1
    if len(true_other_tracks) != 0:
        for true_other_track in true_other_tracks:
            if true_other_track in secondary_tracks: tmpMCOthersRecoSV = tmpMCOthersRecoSV + 1
    if len(true_secondary_bb_tracks) != 0:
        for true_secondary_bb_track in true_secondary_bb_tracks:
            if true_secondary_bb_track in secondary_tracks: tmpMCBottomRecoSV = tmpMCBottomRecoSV + 1
            if true_secondary_bb_track in chains: tmpMCBottomRecoSVSameChain = tmpMCBottomRecoSVSameChain + 1
            if true_secondary_bb_track in particles: tmpMCBottomRecoSVSameParticle = tmpMCBottomRecoSVSameParticle + 1
    if len(true_secondary_cc_tracks) != 0:
        for true_secondary_cc_track in true_secondary_cc_tracks:
            if true_secondary_cc_track in secondary_tracks: tmpMCCharmRecoSV = tmpMCCharmRecoSV + 1
            if true_secondary_cc_track in chains: tmpMCCharmRecoSVSameChain = tmpMCCharmRecoSVSameChain + 1
            if true_secondary_cc_track in particles: tmpMCCharmRecoSVSameParticle = tmpMCCharmRecoSVSameParticle + 1

    print(pycolor.ACCENT + "----------------------------------------------------------------" + pycolor.END)
    if len(true_primary_tracks) != 0:
        tmp_score = tmpMCPrimaryRecoSV/len(true_primary_tracks)
        print(pycolor.RED + "MC Primary / Reco SV : " + str(tmp_score) + pycolor.END)
        NumPVEvent = NumPVEvent + 1
        MCPrimaryRecoSV = MCPrimaryRecoSV + tmp_score
        MCPrimaryRecoSVTrack = MCPrimaryRecoSVTrack + tmpMCPrimaryRecoSV
        NumPVTrack = NumPVTrack + len(true_primary_tracks)
    else: print(pycolor.RED + "MC Primary / Reco SV : [not exists]" + pycolor.END)

    if len(true_other_tracks) != 0:
        tmp_score = tmpMCOthersRecoSV/len(true_other_tracks)
        print(pycolor.RED + "MC Others / Reco SV : " + str(tmp_score) + pycolor.END)
        NumOthersEvent = NumOthersEvent + 1
        MCOthersRecoSV = MCOthersRecoSV + tmp_score
        MCOthersRecoSVTrack = MCOthersRecoSVTrack + tmpMCOthersRecoSV
        NumOthersTrack = NumOthersTrack + len(true_other_tracks)
    else: print(pycolor.RED + "MC Others / Reco SV : [not exists]" + pycolor.END)

    if len(true_secondary_bb_tracks) != 0:
        tmp_score, tmp_chain, tmp_particle = tmpMCBottomRecoSV/len(true_secondary_bb_tracks), tmpMCBottomRecoSVSameChain/len(true_secondary_bb_tracks), tmpMCBottomRecoSVSameParticle/len(true_secondary_bb_tracks)
        print(pycolor.RED + "MC Bottom  / Reco SV : " + str(tmp_score) + " Same Chain : " + str(tmp_chain) + " Same Particle : " + str(tmp_particle) + pycolor.END)
        NumBBEvent = NumBBEvent + 1
        MCBottomRecoSV = MCBottomRecoSV + tmp_score
        MCBottomRecoSVSameChain = MCBottomRecoSVSameChain + tmp_chain
        MCBottomRecoSVSameParticle = MCBottomRecoSVSameParticle + tmp_particle
        MCBottomRecoSVTrack = MCBottomRecoSVTrack + tmpMCBottomRecoSV
        MCBottomRecoSVSameChainTrack = MCBottomRecoSVSameChainTrack + tmpMCBottomRecoSVSameChain
        MCBottomRecoSVSameParticleTrack = MCBottomRecoSVSameParticleTrack + tmpMCBottomRecoSVSameParticle
        NumBBTrack = NumBBTrack + len(true_secondary_bb_tracks)
    else: print(pycolor.RED + "MC Bottom  / Reco SV : [not exists] Same Chain : [not exists] Same Particle : [not exists]" + pycolor.END)
        
    if len(true_secondary_cc_tracks) != 0:
        tmp_score, tmp_chain, tmp_particle = tmpMCCharmRecoSV/len(true_secondary_cc_tracks), tmpMCCharmRecoSVSameChain/len(true_secondary_cc_tracks), tmpMCCharmRecoSVSameParticle/len(true_secondary_cc_tracks)
        print(pycolor.RED + "MC Charm   / Reco SV : " + str(tmp_score)  + " Same Chain : " + str(tmp_chain) + " Same Particle : " + str(tmp_particle) + pycolor.END)
        NumCCEvent = NumCCEvent + 1
        MCCharmRecoSV = MCCharmRecoSV + tmp_score
        MCCharmRecoSVSameChain = MCCharmRecoSVSameChain + tmp_chain
        MCCharmRecoSVSameParticle = MCCharmRecoSVSameParticle + tmp_particle
        MCCharmRecoSVTrack = MCCharmRecoSVTrack + tmpMCCharmRecoSV
        MCCharmRecoSVSameChainTrack = MCCharmRecoSVSameChainTrack + tmpMCCharmRecoSVSameChain
        MCCharmRecoSVSameParticleTrack = MCCharmRecoSVSameParticleTrack + tmpMCCharmRecoSVSameParticle
        NumCCTrack = NumCCTrack + len(true_secondary_cc_tracks)
    else: print(pycolor.RED + "MC Charm   / Reco SV : [not exists] Same Chain : [not exists] Same Particle : [not exists]" + pycolor.END)
    print(pycolor.ACCENT + "-------------------------------------------------------------------------------------------------" + pycolor.END)
		
    return MCPrimaryRecoSV, MCOthersRecoSV, MCBottomRecoSV, MCBottomRecoSVSameChain, MCBottomRecoSVSameParticle, MCCharmRecoSV, MCCharmRecoSVSameChain, MCCharmRecoSVSameParticle, NumPVEvent, NumOthersEvent, NumBBEvent, NumCCEvent, MCPrimaryRecoSVTrack, MCOthersRecoSVTrack, MCBottomRecoSVTrack, MCBottomRecoSVSameChainTrack, MCBottomRecoSVSameParticleTrack, MCCharmRecoSVTrack, MCCharmRecoSVSameChainTrack, MCCharmRecoSVSameParticleTrack, NumPVTrack, NumOthersTrack, NumBBTrack, NumCCTrack


def PrintFinish(MCPrimaryRecoSV, MCOthersRecoSV, MCBottomRecoSV, MCBottomRecoSVSameChain, MCBottomRecoSVSameParticle, MCCharmRecoSV, MCCharmRecoSVSameChain, MCCharmRecoSVSameParticle,
                NumPVEvent, NumCCEvent, NumBBEvent, NumOthersEvent):

    if(NumPVEvent>0): print(pycolor.RED + "MC Primary / Reco SV : " + str(MCPrimaryRecoSV/NumPVEvent) + pycolor.END)
    else: print(pycolor.RED + "MC Primary / Reco SV : [not exist]" + pycolor.END)

    if(NumOthersEvent>0): print(pycolor.RED + "MC Others  / Reco SV : " + str(MCOthersRecoSV/NumOthersEvent) + pycolor.END)
    else: print(pycolor.RED + "MC Others / Reco SV : [not exist]" + pycolor.END)

    if(NumBBEvent>0):
        print(pycolor.RED + "MC Bottom  / Reco SV : " + str(MCBottomRecoSV/NumBBEvent) + " Same Chain : " + str(MCBottomRecoSVSameChain/NumBBEvent) + " Same Particle : " + str(MCBottomRecoSVSameParticle/NumBBEvent) + pycolor.END)
    else: print(pycolor.RED + "MC Bottom  / Reco SV : [not exists] Same Chain : [not exists] Same Particle : [not exists]" + pycolor.END)

    if(NumCCEvent>0):
        print(pycolor.RED + "MC Charm   / Reco SV : " + str(MCCharmRecoSV/NumCCEvent) + " Same Chain : " + str(MCCharmRecoSVSameChain/NumCCEvent) + " Same Particle : " + str(MCCharmRecoSVSameParticle/NumCCEvent) + pycolor.END)
    else: print(pycolor.RED + "MC Charm   / Reco SV : [not exists] Same Chain : [not exists] Same Particle : [not exists]" + pycolor.END)


def PrintFinishTrackBase(MCPrimaryRecoSVTrack, MCOthersRecoSVTrack, MCBottomRecoSVTrack, MCBottomRecoSVSameChainTrack, MCBottomRecoSVSameParticleTrack, 
                         MCCharmRecoSVTrack, MCCharmRecoSVSameChainTrack, MCCharmRecoSVSameParticleTrack,
                         NumPVTrack, NumCCTrack, NumBBTrack, NumOthersTrack):

    if(NumPVTrack>0): print(pycolor.RED + "MC " + str(NumPVTrack) + " MC Primary / Reco SV : " + str(MCPrimaryRecoSVTrack/NumPVTrack) + pycolor.END)
    else: print(pycolor.RED + "MC Primary / Reco SV : [not exist]" + pycolor.END)

    if(NumOthersTrack>0): print(pycolor.RED + "MC " + str(NumOthersTrack) + " MC Others  / Reco SV : " + str(MCOthersRecoSVTrack/NumOthersTrack) + pycolor.END)
    else: print(pycolor.RED + "MC Others / Reco SV : [not exist]" + pycolor.END)

    if(NumBBTrack>0):
        print(pycolor.RED + "MC " + str(NumBBTrack) + " MC Bottom  / Reco SV : " + str(MCBottomRecoSVTrack/NumBBTrack) + " Same Chain : " + str(MCBottomRecoSVSameChainTrack/NumBBTrack) + " Same Particle : " + str(MCBottomRecoSVSameParticleTrack/NumBBTrack) + pycolor.END)
    else: print(pycolor.RED + "MC Bottom  / Reco SV : [not exists] Same Chain : [not exists] Same Particle : [not exists]" + pycolor.END)

    if(NumCCTrack>0):
        print(pycolor.RED + "MC " + str(NumCCTrack) + " MC Charm   / Reco SV : " + str(MCCharmRecoSVTrack/NumCCTrack) + " Same Chain : " + str(MCCharmRecoSVSameChainTrack/NumCCTrack) + " Same Particle : " + str(MCCharmRecoSVSameParticleTrack/NumCCTrack) + pycolor.END)
    else: print(pycolor.RED + "MC Charm   / Reco SV : [not exists] Same Chain : [not exists] Same Particle : [not exists]" + pycolor.END)
\end{lstlisting}

\begin{lstlisting}[caption=崩壊点検出アルゴリズム, label=VertexFinder2]
if __name__ == "__main__":

    #MaxEvent = 1000
    MaxSample = -1
    MaxTrack = 60
    MaxPrimaryVertexLoop = 3
    ThresholdPairSecondaryScoreBBCC = 0.6
    ThresholdPairSecondaryScore = 0.88
    ThresholdPairPosScore = 30
    ThresholdPrimaryScore = 0.55
    ThresholdSecondaryScore = 0.70
    debug = False

    NumPVEvent = 0
    NumOthersEvent = 0
    NumBBEvent = 0
    NumCCEvent = 0

    MCPrimaryRecoSV = 0
    MCOthersRecoSV = 0

    MCBottomRecoSV = 0
    MCBottomRecoSVSameChain = 0
    MCBottomRecoSVSameParticle = 0
    MCCharmRecoSV = 0
    MCCharmRecoSVSameChain = 0
    MCCharmRecoSVSameParticle = 0

    MCPrimaryRecoSVTrack = 0
    MCOthersRecoSVTrack = 0
    MCBottomRecoSVTrack = 0
    MCBottomRecoSVSameChainTrack = 0
    MCBottomRecoSVSameParticleTrack = 0
    MCCharmRecoSVTrack = 0
    MCCharmRecoSVSameChainTrack = 0
    MCCharmRecoSVSameParticleTrack = 0
    
    NumPVTrack = 0
    NumOthersTrack = 0 
    NumBBTrack = 0
    NumCCTrack = 0

    data_path = "data/numpy/Vertex_Finder_bb08_reshaped.npy"

    pair_model_name = "Pair_Model_Standard"

    lstm_model_name = "VLSTM_Model_Standard_PV"
    slstm_model_name = "VLSTM_Model_Standard_SV"

    print("Data Loading ...")
    data = np.load(data_path)
    variables = data[:MaxSample, 3:47]
    
    MaxEvent = int(data[-1, 0])

    print("Model Loading ...")
    pair_model, lstm_model, slstm_model = tools.ModelsLoad(pair_model_name, lstm_model_name, slstm_model_name)

    pred_vertex, pred_position = tools.PairInference(pair_model, variables)
    data = np.concatenate([data[:MaxSample], pred_vertex], 1)
    data = np.concatenate([data, pred_position], 1) # -8:NC, -7:PV, -6:SVCC, -5:SVBB, -4:TVCC, -3:SVBC, -2:Others, -1:Position

    vertices_list = []
    for ievent in range(MaxEvent):
        print("============================================================")
        print("EVENT NUMBER " + str(ievent))
        print("============================================================")
        event_data = [datum for datum in data if datum[0]==ievent]
        event_data = np.array(event_data)
        NTrack = (1 + np.sqrt(1 + 8*event_data.shape[0]))/2 
        
        if debug==True: print("The Number of Tracks in this event is " + str(NTrack))

        # Making Tracks / True Labels 
        encoder_tracks, decoder_tracks, true_label, chain_label = tools.GetEncoderDecoderTracksandTrue(debug, event_data, NTrack, MaxTrack)

        # Secondary Seed Selection
        print("Secondary Seed Selection ...")
        secondary_event_data = tools.SecondarySeedSelectionOne(debug, event_data, ThresholdPairSecondaryScore, ThresholdPairPosScore)
        #secondary_seed_data = SecondarySeedSelectionTwo(debug, secondary_event_data, )

        # Primary Vertex Finder 
        print("Primary Vertex Prediction ...")
        primary_track_list, bigger_primary_scores = tools.PrimaryVertexFinder(debug, MaxPrimaryVertexLoop, ThresholdPrimaryScore, event_data, encoder_tracks, decoder_tracks, lstm_model)

        # Secondary Vertex Finder 
        print("Secondary Vertex Prediction ...")
        primary_track_list, secondary_track_lists = tools.SecondaryVertexFinder(debug, ThresholdSecondaryScore, bigger_primary_scores, primary_track_list, secondary_event_data, encoder_tracks, decoder_tracks, slstm_model);

        # Result 
        print("Finish !!")
        true_primary_tracks, true_secondary_bb_track_lists, true_secondary_cc_track_lists, true_secondary_same_chain_track_lists, true_other_tracks = tools.CountPrintTrueTrackLists(debug, true_label, chain_label)
        
        tools.PrintPredTrackLists(primary_track_list, secondary_track_lists)

        MCPrimaryRecoSV, MCOthersRecoSV, MCBottomRecoSV, MCBottomRecoSVSameChain, MCBottomRecoSVSameParticle, MCCharmRecoSV, MCCharmRecoSVSameChain, MCCharmRecoSVSameParticle, NumPVEvent, NumOthersEvent, NumBBEvent, NumCCEvent, MCPrimaryRecoSVTrack, MCOthersRecoSVTrack, MCBottomRecoSVTrack, MCBottomRecoSVSameChainTrack, MCBottomRecoSVSameParticleTrack, MCCharmRecoSVTrack, MCCharmRecoSVSameChainTrack, MCCharmRecoSVSameParticleTrack, NumPVTrack, NumOthersTrack, NumBBTrack, NumCCTrack
	= tools.EvalPrintResults(debug, secondary_track_lists, true_primary_tracks, true_secondary_bb_track_lists, true_secondary_cc_track_lists, true_secondary_same_chain_track_lists, true_other_tracks, MCPrimaryRecoSV, MCOthersRecoSV, MCBottomRecoSV, MCBottomRecoSVSameChain, MCBottomRecoSVSameParticle, MCCharmRecoSV, MCCharmRecoSVSameChain, MCCharmRecoSVSameParticle, NumPVEvent, NumOthersEvent, NumBBEvent, NumCCEvent, MCPrimaryRecoSVTrack, MCOthersRecoSVTrack, MCBottomRecoSVTrack, MCBottomRecoSVSameChainTrack, MCBottomRecoSVSameParticleTrack, MCCharmRecoSVTrack, MCCharmRecoSVSameChainTrack, MCCharmRecoSVSameParticleTrack, NumPVTrack, NumOthersTrack, NumBBTrack, NumCCTrack)


    tools.PrintFinish(MCPrimaryRecoSV, MCOthersRecoSV, MCBottomRecoSV, MCBottomRecoSVSameChain, MCBottomRecoSVSameParticle, MCCharmRecoSV, MCCharmRecoSVSameChain, MCCharmRecoSVSameParticle, NumPVEvent, NumCCEvent, NumBBEvent, NumOthersEvent)
    tools.PrintFinishTrackBase(MCPrimaryRecoSVTrack, MCOthersRecoSVTrack, MCBottomRecoSVTrack, MCBottomRecoSVSameChainTrack, MCBottomRecoSVSameParticleTrack, MCCharmRecoSVTrack, MCCharmRecoSVSameChainTrack, MCCharmRecoSVSameParticleTrack, NumPVTrack, NumCCTrack, NumBBTrack, NumOthersTrack)
\end{lstlisting}
    
    
    
    
    
    
    
    
    
    
    
    
    
    